# JWT Configuration
JWT_SECRET=your-secret-key-change-this-in-production-min-32-chars
JWT_EXPIRATION=24h

# Server Configuration
PORT=7042
NODE_ENV=development

# CORS Configuration
# Comma-separated list of allowed origins. Use '*' for all origins (not recommended in production)
# Example: ALLOWED_ORIGINS=https://yourdomain.com,https://app.yourdomain.com
ALLOWED_ORIGINS=http://localhost:7042,http://localhost:3000,http://localhost:3001

# Logging Configuration
LOG_LEVEL=info

# Database Configuration (PostgreSQL)
DB_HOST=localhost
DB_PORT=5432
DB_NAME=workstation_saas
DB_USER=postgres
DB_PASSWORD=your-database-password-here

# GitHub Configuration
# Personal Access Token for GitHub API operations (required for PR management)
# Generate at: https://github.com/settings/tokens
# Required scopes: repo (full control of private repositories)
GITHUB_TOKEN=your-github-personal-access-token-here

# SQLite Configuration (for Database Agent)
SQLITE_DB_PATH=./data/workstation.db

# LLM Integration Configuration (Optional - BYOK)
# Enable LLM-powered features (workflow generation, agent selection, optimization)
LLM_ENABLED=true

# LLM Provider: openai, anthropic, ollama, lmstudio, or local
LLM_PROVIDER=openai

# OpenAI Configuration (if using OpenAI)
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-api-key-here
LLM_MODEL=gpt-4

# Anthropic Configuration (if using Claude)
# Get your API key from: https://console.anthropic.com/
# ANTHROPIC_API_KEY=your-anthropic-api-key-here
# LLM_MODEL=claude-3-opus-20240229

# Ollama Configuration (FREE - Local models, works offline)
# Install Ollama from: https://ollama.ai/
# Popular models: llama3, codellama, mistral, qwen2.5, deepseek-coder
# OLLAMA_URL=http://localhost:11434/api
# LLM_MODEL=qwen2.5:32b

# LM Studio Configuration (FREE - Local models, works offline)
# Download from: https://lmstudio.ai/
# Supports: Qwen, Llama, Mistral, and many more
# LMSTUDIO_URL=http://localhost:1234/v1
# LLM_MODEL=qwen2.5-coder-32b-instruct

# Qwen Suite Models (Available via Ollama or LM Studio)
# Qwen2.5 models: qwen2.5:0.5b, qwen2.5:1.5b, qwen2.5:3b, qwen2.5:7b, qwen2.5:14b, qwen2.5:32b, qwen2.5:72b
# Qwen2.5-Coder: qwen2.5-coder:1.5b, qwen2.5-coder:7b, qwen2.5-coder:32b
# Qwen2.5-Math: qwen2.5-math:1.5b, qwen2.5-math:7b, qwen2.5-math:72b

# Custom Local LLM (OpenAI-compatible API)
# LOCAL_LLM_URL=http://localhost:8000
# LLM_MODEL=custom-model-name

# LLM Parameters
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=2000
LLM_RATE_LIMIT=20  # Max LLM API calls per 15 minutes (increase for development)

# Feature Flags for LLM
ENABLE_WORKFLOW_GENERATION=true
ENABLE_AUTO_AGENT_SELECTION=true
ENABLE_ERROR_RECOVERY_SUGGESTIONS=true
ENABLE_WORKFLOW_OPTIMIZATION=true

# AWS S3 Configuration (for S3 Agent)
AWS_ACCESS_KEY_ID=your-aws-access-key-id
AWS_SECRET_ACCESS_KEY=your-aws-secret-access-key
AWS_REGION=us-east-1
AWS_S3_BUCKET=your-s3-bucket-name
# Optional: For S3-compatible services (MinIO, DigitalOcean Spaces, etc.)
# AWS_S3_ENDPOINT=https://your-s3-compatible-endpoint.com
# AWS_S3_FORCE_PATH_STYLE=true
